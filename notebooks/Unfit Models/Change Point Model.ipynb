{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa2fa0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatbu\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "def change_point_model(trial_by_global_param = None, trial_by_logic_param = 'NaN'):\n",
    "    observations = ['Dev at 1', 'Dev at 2','Dev at 3','Dev at 4','Dev at 5','Dev at 6', 'No deviant']\n",
    "\n",
    "    # Step 1: Initialize prior belief (biased towards position 4)\n",
    "    def initialize_priors():\n",
    "        deviant_given_sequence = np.array([1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7])  # Example prior belief\n",
    "        global_prior = np.array([1/3, 1/3, 1/3])\n",
    "        return deviant_given_sequence, global_prior\n",
    "\n",
    "    # Step 2: Initialize likelihood matrices\n",
    "    def initialize_likelihoods():\n",
    "        trial_by_global = trial_by_global_param\n",
    "\n",
    "        if(trial_by_logic_param is None):\n",
    "            trial_logic = np.array([\n",
    "                #HM\n",
    "                [0, 0, 0, 0.33, 0.33, 0.33, 0],\n",
    "                #FC\n",
    "                [0, 0, 0, 0, 0, 0, 1]\n",
    "            ]).T\n",
    "        else:\n",
    "            trial_logic = trial_by_logic_param\n",
    "            \n",
    "        return trial_by_global, trial_logic\n",
    "\n",
    "    # Step 3: Initialize helper functions\n",
    "    def sequence_given_state(sequence, trial_by_global, trial_logic):\n",
    "        deviant_given_state = np.ones(3)\n",
    "        for i in range(len(sequence)):\n",
    "            deviant_given_trial = np.array([trial_logic[sequence[i]][0], trial_logic[sequence[i]][1]])\n",
    "            deviant_given_state *= deviant_given_trial @ trial_by_global\n",
    "\n",
    "        return deviant_given_state\n",
    "\n",
    "    def state_given_sequence(sequence, prior, trial_by_global, trial_logic):\n",
    "        SEQUENCE_GIVEN_STATE = sequence_given_state(sequence, trial_by_global, trial_logic)\n",
    "        state_prob = prior.copy()\n",
    "        state_prob = SEQUENCE_GIVEN_STATE * state_prob\n",
    "        norm_state_prob = state_prob / state_prob.sum()\n",
    "        return norm_state_prob\n",
    "\n",
    "    def future_given_state(trial_by_global, trial_logic):\n",
    "\n",
    "        return trial_logic @ trial_by_global\n",
    "\n",
    "    def future_given_sequence(sequence, prior, future_given_state, trial_by_global, trial_logic):\n",
    "        FUTURE_GIVEN_STATE = future_given_state\n",
    "        STATE_GIVEN_SEQUENCE = state_given_sequence(sequence, prior, trial_by_global, trial_logic)    \n",
    "        \n",
    "        return FUTURE_GIVEN_STATE @ STATE_GIVEN_SEQUENCE.T\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4: Gradually update the prior based on the observed deviant location with recency-based decay\n",
    "    def update_priors_simplified(deviant_given_sequence, global_prior, future_given_state, trial_by_global, trial_logic, deviant_position, obs_history):    \n",
    "        obs_history.append(deviant_position)\n",
    "\n",
    "        size = len(obs_history)\n",
    "        index = (size // 250) * 250 if (size // 250) != 0 else (size // 250) * 250 + 1\n",
    "\n",
    "        clipped_obs_history = obs_history[index - 1: size].copy()\n",
    "\n",
    "        deviant_given_sequence = future_given_sequence(clipped_obs_history, global_prior, future_given_state, trial_by_global, trial_logic)\n",
    "\n",
    "        return deviant_given_sequence, obs_history\n",
    "\n",
    "    # Step 5: Run trials and update beliefs\n",
    "    def run_standard_trials(deviant_given_sequence = None, global_prior = None, belief_over_time = None, obs_history = None, n_trials = 1000):\n",
    "        if (deviant_given_sequence is None or belief_over_time is None or obs_history is None):\n",
    "            belief_over_time = []  # Store belief at each trial\n",
    "            obs_history = []  # Track history of observed deviants\n",
    "        \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence, global_prior = initialize_priors()\n",
    "        else:\n",
    "            belief_over_time = belief_over_time  # Store belief at each trial\n",
    "            obs_history = obs_history  # Track history of observed deviants\n",
    "            \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence = deviant_given_sequence\n",
    "            global_prior = global_prior\n",
    "        \n",
    "        trial_by_global, trial_logic = initialize_likelihoods()\n",
    "        \n",
    "        FUTURE_GIVEN_STATE = future_given_state(trial_by_global, trial_logic)\n",
    "\n",
    "        # Perform the trials\n",
    "        for trial in range(n_trials):        \n",
    "            # Randomly choose where the deviant is located (uniform distribution over 6 positions)\n",
    "            deviant_position = np.random.randint(3, 6)\n",
    "            \n",
    "            # After each trial, update the prior based on the deviant position observed in the current trial (gradual update)\n",
    "            deviant_given_sequence, obs_history = update_priors_simplified(deviant_given_sequence, global_prior, FUTURE_GIVEN_STATE, trial_by_global, trial_logic, deviant_position, obs_history)\n",
    "            \n",
    "            # Record the belief after this trial\n",
    "            belief_over_time.append(deviant_given_sequence.copy())\n",
    "        \n",
    "        return deviant_given_sequence, belief_over_time, obs_history\n",
    "    \n",
    "    # Step 5: Run trials and update beliefs\n",
    "    def run_probe_trials(deviant_given_sequence = None, global_prior = None, belief_over_time = None, obs_history = None, n_trials = 1000):\n",
    "        if (deviant_given_sequence is None or belief_over_time is None or obs_history is None):\n",
    "            belief_over_time = []  # Store belief at each trial\n",
    "            obs_history = []  # Track history of observed deviants\n",
    "        \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence, global_prior = initialize_priors()\n",
    "        else:\n",
    "            belief_over_time = belief_over_time  # Store belief at each trial\n",
    "            obs_history = obs_history  # Track history of observed deviants\n",
    "            \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence = deviant_given_sequence\n",
    "            global_prior = global_prior\n",
    "        \n",
    "        trial_by_global, trial_logic = initialize_likelihoods()\n",
    "        \n",
    "        FUTURE_GIVEN_STATE = future_given_state(trial_by_global, trial_logic)\n",
    "\n",
    "        # Perform the trials\n",
    "        for trial in range(n_trials):        \n",
    "            # Randomly choose where the deviant is located (uniform distribution over 6 positions)\n",
    "            positions = [3, 4, 5, 6]\n",
    "            probabilities = [0.95/3, 0.95/3, 0.95/3, 0.05]  # 60% for positions 3, 4, 5; 40% for position 7\n",
    "            deviant_position = np.random.choice(positions, size=1, p=probabilities)[0]\n",
    "            \n",
    "            # After each trial, update the prior based on the deviant position observed in the current trial (gradual update)\n",
    "            deviant_given_sequence, obs_history = update_priors_simplified(deviant_given_sequence, global_prior, FUTURE_GIVEN_STATE, trial_by_global, trial_logic, deviant_position, obs_history)\n",
    "            \n",
    "            # Record the belief after this trial\n",
    "            belief_over_time.append(deviant_given_sequence.copy())\n",
    "        \n",
    "        return deviant_given_sequence, belief_over_time, obs_history\n",
    "\n",
    "    # Step 5: Run trials and update beliefs\n",
    "    def run_catch_trials(deviant_given_sequence = None, global_prior = None, belief_over_time = None, obs_history = None, n_trials = 1000):\n",
    "        if (deviant_given_sequence is None or belief_over_time is None or obs_history is None):\n",
    "            belief_over_time = []  # Store belief at each trial\n",
    "            obs_history = []  # Track history of observed deviants\n",
    "        \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence, global_prior = initialize_priors()\n",
    "        else:\n",
    "            belief_over_time = belief_over_time  # Store belief at each trial\n",
    "            obs_history = obs_history  # Track history of observed deviants\n",
    "            \n",
    "            # Initialize prior belief (even distribution across all positions initially)\n",
    "            deviant_given_sequence = deviant_given_sequence\n",
    "            global_prior = global_prior\n",
    "        \n",
    "        trial_by_global, trial_logic = initialize_likelihoods()\n",
    "        \n",
    "        FUTURE_GIVEN_STATE = future_given_state(trial_by_global, trial_logic)\n",
    "\n",
    "        # Perform the trials\n",
    "        for trial in range(n_trials):        \n",
    "            positions = [3, 4, 5, 6]\n",
    "            probabilities = [0.6/3, 0.6/3, 0.6/3, 0.4]  # 60% for positions 3, 4, 5; 40% for position 7\n",
    "            deviant_position = np.random.choice(positions, size=1, p=probabilities)[0]\n",
    "            \n",
    "            # After each trial, update the prior based on the deviant position observed in the current trial (gradual update)\n",
    "            deviant_given_sequence, obs_history = update_priors_simplified(deviant_given_sequence, global_prior, FUTURE_GIVEN_STATE, trial_by_global, trial_logic, deviant_position, obs_history)\n",
    "            \n",
    "            # Record the belief after this trial\n",
    "            belief_over_time.append(deviant_given_sequence.copy())\n",
    "        \n",
    "        return deviant_given_sequence, belief_over_time, obs_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    standard_prior_belief, standard_belief_over_time, standard_obs_history = run_standard_trials(None, None, None, None, n_trials = 1500)\n",
    "\n",
    "    global_prior = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "    for i in range(5):\n",
    "        standard_prior_belief, standard_belief_over_time, standard_obs_history = run_standard_trials(standard_prior_belief.copy(), global_prior, standard_belief_over_time.copy(), standard_obs_history.copy(), n_trials = 250)\n",
    "        probe_prior_belief, probe_belief_over_time, probe_obs_history = run_probe_trials(standard_prior_belief.copy(), global_prior, standard_belief_over_time.copy(), standard_obs_history.copy(), n_trials = 250)\n",
    "        standard_prior_belief, standard_belief_over_time, standard_obs_history = run_standard_trials(probe_prior_belief.copy(), global_prior, probe_belief_over_time.copy(), probe_obs_history.copy(), n_trials = 500)\n",
    "        probe_prior_belief, probe_belief_over_time, probe_obs_history = run_probe_trials(standard_prior_belief.copy(), global_prior, standard_belief_over_time.copy(), standard_obs_history.copy(), n_trials = 250)\n",
    "        standard_prior_belief, standard_belief_over_time, standard_obs_history = run_standard_trials(probe_prior_belief.copy(), global_prior, probe_belief_over_time.copy(), probe_obs_history.copy(), n_trials = 250)\n",
    "\n",
    "    catch_prior_belief, catch_belief_over_time, catch_deviant_positions = run_catch_trials(probe_prior_belief.copy(), global_prior, probe_belief_over_time.copy(), probe_obs_history.copy(), n_trials = 1500)\n",
    "\n",
    "    return catch_belief_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca062b9",
   "metadata": {},
   "source": [
    "# Un/Bounded HM/FC & A/Symmetric HM/FC.  Not included stochastic block & no HM free variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24c476",
   "metadata": {},
   "source": [
    "### Bounds on HM/FC & Symmetric HM/FC, No Stochastic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 3. ... 6. 6. 5.]\n",
      "|   iter    |  target   | hm_sta... | hm_probe  | hm_catch  |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-12407.17\u001b[39m | \u001b[39m0.6835246\u001b[39m | \u001b[39m0.9563428\u001b[39m | \u001b[39m0.5927975\u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-13870.61\u001b[39m | \u001b[39m0.7933426\u001b[39m | \u001b[39m0.5748889\u001b[39m | \u001b[39m0.3623978\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-12753.93\u001b[39m | \u001b[39m0.5284609\u001b[39m | \u001b[39m0.9157645\u001b[39m | \u001b[39m0.5404460\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-13357.29\u001b[39m | \u001b[39m0.8469555\u001b[39m | \u001b[39m0.5098805\u001b[39m | \u001b[39m0.6879639\u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-12783.70\u001b[39m | \u001b[39m0.9078968\u001b[39m | \u001b[39m0.6019227\u001b[39m | \u001b[39m0.3727299\u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-15574.36\u001b[39m | \u001b[39m0.5898682\u001b[39m | \u001b[39m0.6460362\u001b[39m | \u001b[39m0.5099025\u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-14752.68\u001b[39m | \u001b[39m0.7116530\u001b[39m | \u001b[39m0.6397899\u001b[39m | \u001b[39m0.5447411\u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-15653.62\u001b[39m | \u001b[39m0.5683519\u001b[39m | \u001b[39m0.6402294\u001b[39m | \u001b[39m0.4465447\u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-13065.46\u001b[39m | \u001b[39m0.7234742\u001b[39m | \u001b[39m0.8768844\u001b[39m | \u001b[39m0.3798695\u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-13973.67\u001b[39m | \u001b[39m0.7519748\u001b[39m | \u001b[39m0.7843589\u001b[39m | \u001b[39m0.3185801\u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-12874.46\u001b[39m | \u001b[39m0.8961147\u001b[39m | \u001b[39m0.6654977\u001b[39m | \u001b[39m0.6250920\u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m-12314.30\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.5026171\u001b[39m |\n",
      "| \u001b[35m13       \u001b[39m | \u001b[35m-12230.07\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.7      \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-12374.25\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-12355.18\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.4976660\u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-12526.58\u001b[39m | \u001b[39m0.5844448\u001b[39m | \u001b[39m0.9429061\u001b[39m | \u001b[39m0.4691930\u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-12243.64\u001b[39m | \u001b[39m0.9857296\u001b[39m | \u001b[39m0.9729563\u001b[39m | \u001b[39m0.6960733\u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-12620.36\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-12357.77\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-12290.33\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-12384.58\u001b[39m | \u001b[39m0.8100239\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.4573706\u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-12405.38\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.7632990\u001b[39m | \u001b[39m0.4838491\u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-12301.88\u001b[39m | \u001b[39m0.7974397\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-12295.48\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.6379646\u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-14277.99\u001b[39m | \u001b[39m0.6652672\u001b[39m | \u001b[39m0.7538960\u001b[39m | \u001b[39m0.5516947\u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-12281.17\u001b[39m | \u001b[39m0.6721188\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-12318.20\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.7921791\u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-12291.28\u001b[39m | \u001b[39m0.6282912\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-12331.12\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.5327845\u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-12324.94\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.7      \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-12281.09\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.6225551\u001b[39m | \u001b[39m0.5183304\u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-12919.21\u001b[39m | \u001b[39m0.5598969\u001b[39m | \u001b[39m0.8947496\u001b[39m | \u001b[39m0.5348919\u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-12269.21\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.6374031\u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-12312.26\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5901645\u001b[39m | \u001b[39m0.3900088\u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-15862.12\u001b[39m | \u001b[39m0.5291728\u001b[39m | \u001b[39m0.6271852\u001b[39m | \u001b[39m0.4348993\u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m-12878.66\u001b[39m | \u001b[39m0.8982139\u001b[39m | \u001b[39m0.5586330\u001b[39m | \u001b[39m0.3603118\u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m-12287.52\u001b[39m | \u001b[39m0.6691777\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.4048504\u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m-12285.90\u001b[39m | \u001b[39m0.8974106\u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.6021325\u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m-12655.94\u001b[39m | \u001b[39m0.7216270\u001b[39m | \u001b[39m0.9272446\u001b[39m | \u001b[39m0.3460520\u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m-12233.50\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.6120502\u001b[39m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import math\n",
    "\n",
    "def standard_obs(data, trial_count):\n",
    "    for i in range(trial_count):\n",
    "        deviant_position = np.random.randint(3, 6)\n",
    "        data = np.append(data, deviant_position)\n",
    "    return data\n",
    "\n",
    "def probe_obs(data, trial_count):\n",
    "    for i in range(trial_count):\n",
    "        positions = [3, 4, 5, 6]\n",
    "        probabilities = [0.95/3, 0.95/3, 0.95/3, 0.05]  \n",
    "        deviant_position = np.random.choice(positions, size=1, p=probabilities)[0]\n",
    "        data = np.append(data, deviant_position)\n",
    "    return data\n",
    "\n",
    "def catch_obs(data, trial_count):\n",
    "    for i in range(trial_count):\n",
    "        positions = [3, 4, 5, 6]\n",
    "        probabilities = [0.6/3, 0.6/3, 0.6/3, 0.4] \n",
    "        deviant_position = np.random.choice(positions, size=1, p=probabilities)[0]\n",
    "        data = np.append(data, deviant_position)\n",
    "    return data\n",
    "\n",
    "data = np.array([])\n",
    "\n",
    "data = standard_obs(data, 1500)\n",
    "for i in range(5):\n",
    "    data = standard_obs(data, 250)\n",
    "    data = probe_obs(data, 250)\n",
    "    data = standard_obs(data, 500)\n",
    "    data = probe_obs(data, 250)\n",
    "    data = standard_obs(data, 250)\n",
    "data = catch_obs(data, 1500)\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n",
    "def neg_log_likelihood(hm_standard, hm_probe, hm_catch):\n",
    "    trial_by_global = np.array([\n",
    "        [hm_standard, 1 - hm_standard],\n",
    "        [hm_probe, 1 - hm_probe],\n",
    "        [hm_catch, 1 - hm_catch]\n",
    "    ]).T\n",
    "\n",
    "    priors = change_point_model(trial_by_global)\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(len(priors)): ## priors and data must be same len\n",
    "        nll += -math.log(priors[i][int(data[i])])\n",
    "\n",
    "    return -nll #have to negate since bayesian optimizer is a maximizer not a minimizer\n",
    "\n",
    "# Define bounds for each parameter (between 0.01 and 0.99 to avoid degenerate values)\n",
    "pbounds = {\n",
    "    'hm_standard': (0.5, 0.99),\n",
    "    'hm_probe': (0.5, 0.98),\n",
    "    'hm_catch': (0.3, 0.7)\n",
    "}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=neg_log_likelihood,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # random samples before GP fitting\n",
    "    n_iter=30        # number of iterations of Bayesian optimization\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132991c",
   "metadata": {},
   "source": [
    "### Bounds on HM/FC & Asymmetric HM/FC, No Stochastic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfde5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hm_sta... | hm_probe  | hm_catch  | fc_sta... | fc_probe  | fc_catch  |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-12357.75\u001b[39m | \u001b[39m0.6835246\u001b[39m | \u001b[39m0.9563428\u001b[39m | \u001b[39m0.5927975\u001b[39m | \u001b[39m0.3033426\u001b[39m | \u001b[39m0.0864491\u001b[39m | \u001b[39m0.0864373\u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-12895.02\u001b[39m | \u001b[39m0.5284609\u001b[39m | \u001b[39m0.9157645\u001b[39m | \u001b[39m0.5404460\u001b[39m | \u001b[39m0.3569555\u001b[39m | \u001b[39m0.0200864\u001b[39m | \u001b[39m0.4852558\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-13113.02\u001b[39m | \u001b[39m0.9078968\u001b[39m | \u001b[39m0.6019227\u001b[39m | \u001b[39m0.3727299\u001b[39m | \u001b[39m0.0998682\u001b[39m | \u001b[39m0.1590786\u001b[39m | \u001b[39m0.2671306\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-15346.19\u001b[39m | \u001b[39m0.7116530\u001b[39m | \u001b[39m0.6397899\u001b[39m | \u001b[39m0.5447411\u001b[39m | \u001b[39m0.0783519\u001b[39m | \u001b[39m0.1531508\u001b[39m | \u001b[39m0.1895173\u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-12720.50\u001b[39m | \u001b[39m0.7234742\u001b[39m | \u001b[39m0.8768844\u001b[39m | \u001b[39m0.3798695\u001b[39m | \u001b[39m0.2619748\u001b[39m | \u001b[39m0.3002831\u001b[39m | \u001b[39m0.0327607\u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-13345.82\u001b[39m | \u001b[39m0.7976969\u001b[39m | \u001b[39m0.5818515\u001b[39m | \u001b[39m0.3260206\u001b[39m | \u001b[39m0.4749539\u001b[39m | \u001b[39m0.4831596\u001b[39m | \u001b[39m0.4061147\u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-15802.38\u001b[39m | \u001b[39m0.6492607\u001b[39m | \u001b[39m0.5468826\u001b[39m | \u001b[39m0.5736932\u001b[39m | \u001b[39m0.2256747\u001b[39m | \u001b[39m0.0697987\u001b[39m | \u001b[39m0.2526366\u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-12468.55\u001b[39m | \u001b[39m0.5168503\u001b[39m | \u001b[39m0.9364737\u001b[39m | \u001b[39m0.4035119\u001b[39m | \u001b[39m0.3346359\u001b[39m | \u001b[39m0.1627384\u001b[39m | \u001b[39m0.2648333\u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-13844.94\u001b[39m | \u001b[39m0.7678880\u001b[39m | \u001b[39m0.5887301\u001b[39m | \u001b[39m0.6878338\u001b[39m | \u001b[39m0.3898150\u001b[39m | \u001b[39m0.4703544\u001b[39m | \u001b[39m0.4484654\u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-12986.76\u001b[39m | \u001b[39m0.7929709\u001b[39m | \u001b[39m0.9424996\u001b[39m | \u001b[39m0.3353970\u001b[39m | \u001b[39m0.1060316\u001b[39m | \u001b[39m0.0321613\u001b[39m | \u001b[39m0.1694118\u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m-11783.44\u001b[39m | \u001b[35m0.7526542\u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.3881559\u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.1018149\u001b[39m | \u001b[35m0.1499655\u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m-11329.85\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.3      \u001b[39m | \u001b[35m0.4282228\u001b[39m | \u001b[35m0.2060713\u001b[39m | \u001b[35m0.3713153\u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-11343.23\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.1365950\u001b[39m | \u001b[39m0.4434054\u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-16534.12\u001b[39m | \u001b[39m0.5001670\u001b[39m | \u001b[39m0.5389395\u001b[39m | \u001b[39m0.5774879\u001b[39m | \u001b[39m0.0143756\u001b[39m | \u001b[39m0.4948720\u001b[39m | \u001b[39m0.3810291\u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m-11238.56\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.7      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.1527034\u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-14361.65\u001b[39m | \u001b[39m0.7196604\u001b[39m | \u001b[39m0.7688935\u001b[39m | \u001b[39m0.3646590\u001b[39m | \u001b[39m0.1407192\u001b[39m | \u001b[39m0.1650517\u001b[39m | \u001b[39m0.1201212\u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m-11231.84\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.98     \u001b[39m | \u001b[35m0.3      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.01     \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-11239.29\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1732758\u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-11332.72\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-13532.65\u001b[39m | \u001b[39m0.8216860\u001b[39m | \u001b[39m0.5459206\u001b[39m | \u001b[39m0.6721687\u001b[39m | \u001b[39m0.2239020\u001b[39m | \u001b[39m0.0301599\u001b[39m | \u001b[39m0.1730568\u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-11236.95\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-14590.54\u001b[39m | \u001b[39m0.7231310\u001b[39m | \u001b[39m0.6475906\u001b[39m | \u001b[39m0.4630487\u001b[39m | \u001b[39m0.2954562\u001b[39m | \u001b[39m0.2217737\u001b[39m | \u001b[39m0.1134767\u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-13953.27\u001b[39m | \u001b[39m0.7587802\u001b[39m | \u001b[39m0.5156077\u001b[39m | \u001b[39m0.5155505\u001b[39m | \u001b[39m0.3896976\u001b[39m | \u001b[39m0.4140515\u001b[39m | \u001b[39m0.0927709\u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-12032.96\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-11291.99\u001b[39m | \u001b[39m0.9846706\u001b[39m | \u001b[39m0.9744795\u001b[39m | \u001b[39m0.6846576\u001b[39m | \u001b[39m0.4993865\u001b[39m | \u001b[39m0.4881838\u001b[39m | \u001b[39m0.1722631\u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-12475.14\u001b[39m | \u001b[39m0.5363697\u001b[39m | \u001b[39m0.9387850\u001b[39m | \u001b[39m0.4468552\u001b[39m | \u001b[39m0.3191538\u001b[39m | \u001b[39m0.1543572\u001b[39m | \u001b[39m0.2316834\u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-11273.71\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-11264.35\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-11546.53\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.3022008\u001b[39m | \u001b[39m0.3093605\u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-16022.64\u001b[39m | \u001b[39m0.6286216\u001b[39m | \u001b[39m0.5266535\u001b[39m | \u001b[39m0.3406480\u001b[39m | \u001b[39m0.1093379\u001b[39m | \u001b[39m0.4411025\u001b[39m | \u001b[39m0.1904812\u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-11233.14\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-11233.34\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-11233.06\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-11324.05\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[35m35       \u001b[39m | \u001b[35m-11226.36\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.3      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m0.5      \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m-11231.67\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m-11236.96\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.7663338\u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.2991378\u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m-11273.51\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m-11333.24\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m-11326.05\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(hm_standard, hm_probe, hm_catch, fc_standard, fc_probe, fc_catch):\n",
    "    trial_by_global = np.array([\n",
    "        [hm_standard, fc_standard],\n",
    "        [hm_probe, fc_probe],\n",
    "        [hm_catch, fc_catch]\n",
    "    ]).T\n",
    "\n",
    "    priors = change_point_model(trial_by_global)\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(len(priors)): ## priors and data must be same len\n",
    "        nll += -math.log(priors[i][int(data[i])])\n",
    "\n",
    "    return -nll #have to negate since bayesian optimizer is a maximizer not a minimizer\n",
    "\n",
    "# Define bounds for each parameter (between 0.01 and 0.99 to avoid degenerate values)\n",
    "pbounds = {\n",
    "    'hm_standard': (0.5, 0.99),\n",
    "    'hm_probe': (0.5, 0.98),\n",
    "    'hm_catch': (0.3, 0.7),\n",
    "    'fc_standard': (0.01, 0.5),\n",
    "    'fc_probe': (0.01, 0.5),\n",
    "    'fc_catch': (0.01, 0.5)\n",
    "}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=neg_log_likelihood,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # random samples before GP fitting\n",
    "    n_iter=30        # number of iterations of Bayesian optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338bfc2",
   "metadata": {},
   "source": [
    "### Unbounded HM/FC & Symmetric HM/FC, No Stochastic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33016b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hm_sta... | hm_probe  | hm_catch  |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-12563.41\u001b[39m | \u001b[39m0.3770493\u001b[39m | \u001b[39m0.9417000\u001b[39m | \u001b[39m0.7273540\u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-16231.81\u001b[39m | \u001b[39m0.5966853\u001b[39m | \u001b[39m0.1628982\u001b[39m | \u001b[39m0.1628746\u001b[39m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[0;32m     25\u001b[0m     f\u001b[38;5;241m=\u001b[39mneg_log_likelihood,\n\u001b[0;32m     26\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[0;32m     27\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     28\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mmaximize(\n\u001b[0;32m     33\u001b[0m     init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# random samples before GP fitting\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m        \u001b[38;5;66;03m# number of iterations of Bayesian optimization\u001b[39;00m\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Fatbu\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:322\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter)\u001b[0m\n\u001b[0;32m    320\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest()\n\u001b[0;32m    321\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobe(x_probe, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32mc:\\Users\\Fatbu\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:239\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mprobe(params)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_optimization_step(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mkeys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mres()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mparams_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax\n\u001b[0;32m    242\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Fatbu\\anaconda3\\Lib\\site-packages\\bayes_opt\\target_space.py:555\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    553\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target function has been provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m--> 555\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdict_params)\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mneg_log_likelihood\u001b[1;34m(hm_standard, hm_probe, hm_catch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneg_log_likelihood\u001b[39m(hm_standard, hm_probe, hm_catch):\n\u001b[0;32m      2\u001b[0m     trial_by_global \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m      3\u001b[0m         [hm_standard, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m hm_standard],\n\u001b[0;32m      4\u001b[0m         [hm_probe, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m hm_probe],\n\u001b[0;32m      5\u001b[0m         [hm_catch, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m hm_catch]\n\u001b[0;32m      6\u001b[0m     ])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m----> 8\u001b[0m     priors \u001b[38;5;241m=\u001b[39m change_point_model(trial_by_global)\n\u001b[0;32m     10\u001b[0m     nll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(priors)): \u001b[38;5;66;03m## priors and data must be same len\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 184\u001b[0m, in \u001b[0;36mchange_point_model\u001b[1;34m(trial_by_global_param)\u001b[0m\n\u001b[0;32m    182\u001b[0m     probe_prior_belief, probe_belief_over_time, probe_obs_history \u001b[38;5;241m=\u001b[39m run_probe_trials(standard_prior_belief\u001b[38;5;241m.\u001b[39mcopy(), global_prior, standard_belief_over_time\u001b[38;5;241m.\u001b[39mcopy(), standard_obs_history\u001b[38;5;241m.\u001b[39mcopy(), n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m)\n\u001b[0;32m    183\u001b[0m     standard_prior_belief, standard_belief_over_time, standard_obs_history \u001b[38;5;241m=\u001b[39m run_standard_trials(probe_prior_belief\u001b[38;5;241m.\u001b[39mcopy(), global_prior, probe_belief_over_time\u001b[38;5;241m.\u001b[39mcopy(), probe_obs_history\u001b[38;5;241m.\u001b[39mcopy(), n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m--> 184\u001b[0m     probe_prior_belief, probe_belief_over_time, probe_obs_history \u001b[38;5;241m=\u001b[39m run_probe_trials(standard_prior_belief\u001b[38;5;241m.\u001b[39mcopy(), global_prior, standard_belief_over_time\u001b[38;5;241m.\u001b[39mcopy(), standard_obs_history\u001b[38;5;241m.\u001b[39mcopy(), n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m)\n\u001b[0;32m    185\u001b[0m     standard_prior_belief, standard_belief_over_time, standard_obs_history \u001b[38;5;241m=\u001b[39m run_standard_trials(probe_prior_belief\u001b[38;5;241m.\u001b[39mcopy(), global_prior, probe_belief_over_time\u001b[38;5;241m.\u001b[39mcopy(), probe_obs_history\u001b[38;5;241m.\u001b[39mcopy(), n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m)\n\u001b[0;32m    187\u001b[0m catch_prior_belief, catch_belief_over_time, catch_deviant_positions \u001b[38;5;241m=\u001b[39m run_catch_trials(probe_prior_belief\u001b[38;5;241m.\u001b[39mcopy(), global_prior, probe_belief_over_time\u001b[38;5;241m.\u001b[39mcopy(), probe_obs_history\u001b[38;5;241m.\u001b[39mcopy(), n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 130\u001b[0m, in \u001b[0;36mchange_point_model.<locals>.run_probe_trials\u001b[1;34m(deviant_given_sequence, global_prior, belief_over_time, obs_history, n_trials)\u001b[0m\n\u001b[0;32m    127\u001b[0m deviant_position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(positions, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39mprobabilities)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# After each trial, update the prior based on the deviant position observed in the current trial (gradual update)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m deviant_given_sequence, obs_history \u001b[38;5;241m=\u001b[39m update_priors_simplified(deviant_given_sequence, global_prior, FUTURE_GIVEN_STATE, trial_by_global, trial_logic, deviant_position, obs_history)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Record the belief after this trial\u001b[39;00m\n\u001b[0;32m    133\u001b[0m belief_over_time\u001b[38;5;241m.\u001b[39mappend(deviant_given_sequence\u001b[38;5;241m.\u001b[39mcopy())\n",
      "Cell \u001b[1;32mIn[4], line 65\u001b[0m, in \u001b[0;36mchange_point_model.<locals>.update_priors_simplified\u001b[1;34m(deviant_given_sequence, global_prior, future_given_state, trial_by_global, trial_logic, deviant_position, obs_history)\u001b[0m\n\u001b[0;32m     61\u001b[0m index \u001b[38;5;241m=\u001b[39m (size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m250\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m250\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m250\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m250\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m clipped_obs_history \u001b[38;5;241m=\u001b[39m obs_history[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: size]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 65\u001b[0m deviant_given_sequence \u001b[38;5;241m=\u001b[39m future_given_sequence(clipped_obs_history, global_prior, future_given_state, trial_by_global, trial_logic)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deviant_given_sequence, obs_history\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mchange_point_model.<locals>.future_given_sequence\u001b[1;34m(sequence, prior, future_given_state, trial_by_global, trial_logic)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuture_given_sequence\u001b[39m(sequence, prior, future_given_state, trial_by_global, trial_logic):\n\u001b[0;32m     49\u001b[0m     FUTURE_GIVEN_STATE \u001b[38;5;241m=\u001b[39m future_given_state\n\u001b[1;32m---> 50\u001b[0m     STATE_GIVEN_SEQUENCE \u001b[38;5;241m=\u001b[39m state_given_sequence(sequence, prior, trial_by_global, trial_logic)    \n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FUTURE_GIVEN_STATE \u001b[38;5;241m@\u001b[39m STATE_GIVEN_SEQUENCE\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mchange_point_model.<locals>.state_given_sequence\u001b[1;34m(sequence, prior, trial_by_global, trial_logic)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstate_given_sequence\u001b[39m(sequence, prior, trial_by_global, trial_logic):\n\u001b[1;32m---> 38\u001b[0m     SEQUENCE_GIVEN_STATE \u001b[38;5;241m=\u001b[39m sequence_given_state(sequence, trial_by_global, trial_logic)\n\u001b[0;32m     39\u001b[0m     state_prob \u001b[38;5;241m=\u001b[39m prior\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     40\u001b[0m     state_prob \u001b[38;5;241m=\u001b[39m SEQUENCE_GIVEN_STATE \u001b[38;5;241m*\u001b[39m state_prob\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mchange_point_model.<locals>.sequence_given_state\u001b[1;34m(sequence, trial_by_global, trial_logic)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence)):\n\u001b[0;32m     32\u001b[0m     deviant_given_trial \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([trial_logic[sequence[i]][\u001b[38;5;241m0\u001b[39m], trial_logic[sequence[i]][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m---> 33\u001b[0m     deviant_given_state \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m deviant_given_trial \u001b[38;5;241m@\u001b[39m trial_by_global\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deviant_given_state\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(hm_standard, hm_probe, hm_catch):\n",
    "    trial_by_global = np.array([\n",
    "        [hm_standard, 1- hm_standard],\n",
    "        [hm_probe, 1 - hm_probe],\n",
    "        [hm_catch, 1 - hm_catch]\n",
    "    ]).T\n",
    "\n",
    "    priors = change_point_model(trial_by_global)\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(len(priors)): ## priors and data must be same len\n",
    "        nll += -math.log(priors[i][int(data[i])])\n",
    "\n",
    "    return -nll #have to negate since bayesian optimizer is a maximizer not a minimizer\n",
    "\n",
    "# Define bounds for each parameter (between 0.01 and 0.99 to avoid degenerate values)\n",
    "pbounds = {\n",
    "    'hm_standard': (0.01, 0.99),\n",
    "    'hm_probe': (0.01, 0.99),\n",
    "    'hm_catch': (0.01, 0.99)\n",
    "}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=neg_log_likelihood,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # random samples before GP fitting\n",
    "    n_iter=30        # number of iterations of Bayesian optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5cde3",
   "metadata": {},
   "source": [
    "### Unbounded HM/FC & Asymmetric HM/FC, No Stochastic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ad3e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hm_sta... | hm_probe  | hm_catch  | fc_sta... | fc_probe  | fc_catch  |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-12419.67\u001b[39m | \u001b[39m0.3770493\u001b[39m | \u001b[39m0.9417000\u001b[39m | \u001b[39m0.7273540\u001b[39m | \u001b[39m0.5966853\u001b[39m | \u001b[39m0.1628982\u001b[39m | \u001b[39m0.1628746\u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-12942.17\u001b[39m | \u001b[39m0.0669219\u001b[39m | \u001b[39m0.8588526\u001b[39m | \u001b[39m0.5990927\u001b[39m | \u001b[39m0.7039111\u001b[39m | \u001b[39m0.0301728\u001b[39m | \u001b[39m0.9605116\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-13576.38\u001b[39m | \u001b[39m0.8257937\u001b[39m | \u001b[39m0.2180923\u001b[39m | \u001b[39m0.1881884\u001b[39m | \u001b[39m0.1897364\u001b[39m | \u001b[39m0.3081573\u001b[39m | \u001b[39m0.5242613\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-16091.52\u001b[39m | \u001b[39m0.4333061\u001b[39m | \u001b[39m0.2954045\u001b[39m | \u001b[39m0.6096158\u001b[39m | \u001b[39m0.1467039\u001b[39m | \u001b[39m0.2963017\u001b[39m | \u001b[39m0.3690346\u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-13425.39\u001b[39m | \u001b[39m0.4569485\u001b[39m | \u001b[39m0.7794724\u001b[39m | \u001b[39m0.2056803\u001b[39m | \u001b[39m0.5139497\u001b[39m | \u001b[39m0.5905662\u001b[39m | \u001b[39m0.0555214\u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-15561.00\u001b[39m | \u001b[39m0.6053939\u001b[39m | \u001b[39m0.1771136\u001b[39m | \u001b[39m0.0737505\u001b[39m | \u001b[39m0.9399078\u001b[39m | \u001b[39m0.9563193\u001b[39m | \u001b[39m0.8022294\u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-14835.61\u001b[39m | \u001b[39m0.3085214\u001b[39m | \u001b[39m0.1057186\u001b[39m | \u001b[39m0.6805483\u001b[39m | \u001b[39m0.4413494\u001b[39m | \u001b[39m0.1295974\u001b[39m | \u001b[39m0.4952733\u001b[39m |\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m-12412.03\u001b[39m | \u001b[35m0.0437007\u001b[39m | \u001b[35m0.9011339\u001b[39m | \u001b[35m0.2636043\u001b[39m | \u001b[35m0.6592718\u001b[39m | \u001b[35m0.3154768\u001b[39m | \u001b[35m0.5196666\u001b[39m |\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m-11163.47\u001b[39m | \u001b[35m0.5457760\u001b[39m | \u001b[35m0.1911573\u001b[39m | \u001b[35m0.9601929\u001b[39m | \u001b[35m0.7696301\u001b[39m | \u001b[35m0.9307089\u001b[39m | \u001b[35m0.8869308\u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-13137.30\u001b[39m | \u001b[39m0.5959419\u001b[39m | \u001b[39m0.9134367\u001b[39m | \u001b[39m0.0967226\u001b[39m | \u001b[39m0.2020632\u001b[39m | \u001b[39m0.0543227\u001b[39m | \u001b[39m0.3288237\u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m-10804.20\u001b[39m | \u001b[35m0.4157715\u001b[39m | \u001b[35m0.4230915\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.8005960\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-10813.25\u001b[39m | \u001b[39m0.8237726\u001b[39m | \u001b[39m0.5756562\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.8294697\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-10911.23\u001b[39m | \u001b[39m0.4848960\u001b[39m | \u001b[39m0.9783005\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.8911966\u001b[39m | \u001b[39m0.6772061\u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m-10783.61\u001b[39m | \u001b[35m0.9894552\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.3989046\u001b[39m | \u001b[35m0.99     \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m-10781.59\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.2522945\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-10782.10\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.0502461\u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-10793.78\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.0564060\u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m-10776.32\u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.99     \u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m0.99     \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-10788.13\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-10776.51\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.1391804\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-10784.63\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.0100000\u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-10786.67\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-13628.29\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-10796.99\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.4460935\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.4547099\u001b[39m | \u001b[39m0.4332685\u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-10784.02\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-10793.64\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-10787.05\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-10788.83\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-10788.02\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-10787.41\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-13643.80\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-10779.65\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-10795.29\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5586193\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.5681195\u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-10777.51\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-10781.24\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m-10788.07\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m-10794.19\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.4255955\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.4463528\u001b[39m | \u001b[39m0.4077569\u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m-10776.95\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m-10806.81\u001b[39m | \u001b[39m0.5333673\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.5519960\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m-10784.02\u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m0.99     \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(hm_standard, hm_probe, hm_catch, fc_standard, fc_probe, fc_catch):\n",
    "    trial_by_global = np.array([\n",
    "        [hm_standard, fc_standard],\n",
    "        [hm_probe, fc_probe],\n",
    "        [hm_catch, fc_catch]\n",
    "    ]).T\n",
    "\n",
    "    priors = change_point_model(trial_by_global)\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(len(priors)): ## priors and data must be same len\n",
    "        nll += -math.log(priors[i][int(data[i])])\n",
    "\n",
    "    return -nll #have to negate since bayesian optimizer is a maximizer not a minimizer\n",
    "\n",
    "# Define bounds for each parameter (between 0.01 and 0.99 to avoid degenerate values)\n",
    "pbounds = {\n",
    "    'hm_standard': (0.01, 0.99),\n",
    "    'hm_probe': (0.01, 0.99),\n",
    "    'hm_catch': (0.01, 0.99),\n",
    "    'fc_standard': (0.01, 0.99),\n",
    "    'fc_probe': (0.01, 0.99),\n",
    "    'fc_catch': (0.01, 0.99),\n",
    "}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=neg_log_likelihood,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # random samples before GP fitting\n",
    "    n_iter=30        # number of iterations of Bayesian optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac20c7d",
   "metadata": {},
   "source": [
    "# HM Free Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb0de78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianOptimization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     21\u001b[0m pbounds \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhm_standard\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.99\u001b[39m),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhm_probe\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.98\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev5\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.49\u001b[39m)\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Initialize the optimizer\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[0;32m     31\u001b[0m     f\u001b[38;5;241m=\u001b[39mneg_log_likelihood,\n\u001b[0;32m     32\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[0;32m     33\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     34\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mmaximize(\n\u001b[0;32m     39\u001b[0m     init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# random samples before GP fitting\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m        \u001b[38;5;66;03m# number of iterations of Bayesian optimization\u001b[39;00m\n\u001b[0;32m     41\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BayesianOptimization' is not defined"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(hm_standard, hm_probe, hm_catch, dev4, dev5):\n",
    "    trial_by_global = np.array([\n",
    "        [hm_standard, 1 - hm_standard],\n",
    "        [hm_probe, 1 - hm_probe],\n",
    "        [hm_catch, 1 - hm_catch]\n",
    "    ]).T\n",
    "    trial_by_logic = np.array([\n",
    "        [0, 0, 0, dev4, dev5, 1 - dev4 - dev5, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1]\n",
    "    ]).T\n",
    "\n",
    "    priors = change_point_model(trial_by_global, trial_by_logic)\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(len(priors)): ## priors and data must be same len\n",
    "        nll += -math.log(priors[i][int(data[i])])\n",
    "\n",
    "    return -nll #have to negate since bayesian optimizer is a maximizer not a minimizer\n",
    "\n",
    "# Define bounds for each parameter (between 0.01 and 0.99 to avoid degenerate values)\n",
    "pbounds = {\n",
    "    'hm_standard': (0.5, 0.99),\n",
    "    'hm_probe': (0.5, 0.98),\n",
    "    'hm_catch': (0.3, 0.7),\n",
    "    'dev4': (0.01, 0.49),\n",
    "    'dev5': (0.01, 0.49)\n",
    "}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=neg_log_likelihood,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # random samples before GP fitting\n",
    "    n_iter=30        # number of iterations of Bayesian optimization\n",
    ")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
